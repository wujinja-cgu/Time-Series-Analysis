{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":260,"sourceType":"datasetVersion","datasetId":122},{"sourceId":2390192,"sourceType":"datasetVersion","datasetId":1439417}],"dockerImageVersionId":30097,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport sys\n# insert at 1, 0 is the script path (or '' in REPL)\nsys.path.insert(1, '/kaggle/input/transformersscript/')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-30T06:18:10.305314Z","iopub.execute_input":"2021-06-30T06:18:10.305727Z","iopub.status.idle":"2021-06-30T06:18:10.314575Z","shell.execute_reply.started":"2021-06-30T06:18:10.305685Z","shell.execute_reply":"2021-06-30T06:18:10.313493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Investigating Transformers vs RNNs (GRU) using the Power Consumption Dataset\n\n- We start off with importing the dataset, setting a time-series index and converting all values to numeric type","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\nimport numpy as np\nfrom timeseries_preprocessing import timeseries_dataset_from_array\ndf1 = pd.read_csv(\"/kaggle/input/electric-power-consumption-data-set/household_power_consumption.txt\",delimiter=\";\")\ndf1[\"datetime\"] = df1[[\"Date\",\"Time\"]].apply(lambda x: x[0]+\" \"+x[1],axis=1)\ndf1[\"datetime\"] = pd.to_datetime(df1[\"datetime\"], infer_datetime_format=True)\ndf1 = df1.drop([\"Date\",\"Time\"],axis=1)\ndf1.index = df1[\"datetime\"]\ndf1 = df1.drop(\"datetime\",axis=1)\ndf1 = df1.apply(lambda x: pd.to_numeric(x, errors='coerce'))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:18:12.881593Z","iopub.execute_input":"2021-06-30T06:18:12.881983Z","iopub.status.idle":"2021-06-30T06:19:01.35635Z","shell.execute_reply.started":"2021-06-30T06:18:12.881952Z","shell.execute_reply":"2021-06-30T06:19:01.355467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### If you look at the dataset, you will realise that the original sample frequency is 1 minute\n- To simplify the dataset, we resample this to 30 Mins interval\n- We also use this opportunity to normalise the dataset using MinMaxScaler","metadata":{}},{"cell_type":"code","source":"df1","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:19:01.357898Z","iopub.execute_input":"2021-06-30T06:19:01.35823Z","iopub.status.idle":"2021-06-30T06:19:01.383575Z","shell.execute_reply.started":"2021-06-30T06:19:01.358192Z","shell.execute_reply":"2021-06-30T06:19:01.382747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = df1.interpolate()\ndf2 = df1.resample('30T').mean()\ndf2 = df2.interpolate()\ndf2_data = df2.values\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(df2_data)\ndf2_data = scaler.transform(df2_data)\n\nfor i,name in enumerate(df2.columns):\n    df2[name] = df2_data[:,i]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:19:01.385305Z","iopub.execute_input":"2021-06-30T06:19:01.38569Z","iopub.status.idle":"2021-06-30T06:19:02.520865Z","shell.execute_reply.started":"2021-06-30T06:19:01.385637Z","shell.execute_reply":"2021-06-30T06:19:02.520014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can then split the data into training and testing set\n- Shuffle is False to preserve the sequential information (If it is set to True, the rows will be scrambled and the temporal information will be destroyed)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df2.iloc[:,:-3].values, df2.iloc[:,-3:].values, test_size=0.05,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:19:02.522385Z","iopub.execute_input":"2021-06-30T06:19:02.522731Z","iopub.status.idle":"2021-06-30T06:19:02.577275Z","shell.execute_reply.started":"2021-06-30T06:19:02.522693Z","shell.execute_reply":"2021-06-30T06:19:02.57646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We Convert the training data to tf.float32\n- We are going to train the GRU and Transformer models with the tf.GradientTape method\n- casting the data to tensorflow datatype is therefore required","metadata":{}},{"cell_type":"code","source":"data = tf.cast(X_train,tf.float32)\ntargets = tf.cast(y_train,tf.float32)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:19:02.578572Z","iopub.execute_input":"2021-06-30T06:19:02.578957Z","iopub.status.idle":"2021-06-30T06:19:03.966026Z","shell.execute_reply.started":"2021-06-30T06:19:02.578919Z","shell.execute_reply":"2021-06-30T06:19:03.965191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Converting to rolling window dataset\n- 24 historical samples of the first 4 columns (Global_active_power\tGlobal_reactive_power\tVoltage\tGlobal_intensity) will predict the future 6 samples of the last 3 columns (Sub_metering_1\tSub_metering_2\tSub_metering_3)\n- the method \"timeseries_dataset_from_array\" is copied from https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array\n- I placed it in a seperate file because I am using tensorflow 2.0 in my personal computer and \"timeseries_dataset_from_array\" is not present in older versions","metadata":{}},{"cell_type":"code","source":"sample_length = 24\ninput_dataset = timeseries_dataset_from_array(data,None, sequence_length=sample_length,batch_size=256, sequence_stride=sample_length)\ntarget_dataset = timeseries_dataset_from_array(targets, None, sequence_length=6,batch_size=256, sequence_stride=sample_length)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:19:03.967178Z","iopub.execute_input":"2021-06-30T06:19:03.967486Z","iopub.status.idle":"2021-06-30T06:19:04.098154Z","shell.execute_reply.started":"2021-06-30T06:19:03.967458Z","shell.execute_reply":"2021-06-30T06:19:04.097338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining the transformer model\n- The transformer model is neatly imported and initiated as shown below\n- This transformer model is modified from https://www.tensorflow.org/text/tutorials/transformer#create_the_transformer\n- The original version is made for processing NLP tokens\n- I replaced the Embedding layers (Mean for discrete sequential data) with TimeDistributed Layers to process continuous variables instead\n- Here I defined the model so that the number of parameters closely match the GRU model below\n- Instead of going deeper by using 4 layers as shown below, you can go wider by using 2 layers and a larger dff, head, d_model size","metadata":{}},{"cell_type":"code","source":"from transformers import Transformer\nxformer = Transformer(3,num_layers=4,num_heads=4,dff=256,d_model=32)\npred = xformer(tf.random.normal((1,24,4)),tf.random.normal((1,6,3)),False)\n# xformer.load_weights(\"xformer.h5\")\nprint(pred.shape)\nxformer.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:19:04.099396Z","iopub.execute_input":"2021-06-30T06:19:04.099742Z","iopub.status.idle":"2021-06-30T06:19:05.800204Z","shell.execute_reply.started":"2021-06-30T06:19:04.099707Z","shell.execute_reply":"2021-06-30T06:19:05.79938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- ### Because the number of params is similar, this should be a close fight!\n- Im trying to measure the efficiency of the models instead","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import GRU,Dense,Input,TimeDistributed,RepeatVector\nfrom tensorflow.keras.models import Model\n\nin1 = Input((24,4))\ngru1 = GRU(128)(in1)\ngru2 = RepeatVector(6)(gru1)\ngru2 = GRU(128,return_sequences=True)(gru2)\ngru2 = TimeDistributed(Dense(3))(gru2)\n\nmodel = Model(in1,gru2)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:19:05.802237Z","iopub.execute_input":"2021-06-30T06:19:05.802586Z","iopub.status.idle":"2021-06-30T06:19:06.169883Z","shell.execute_reply.started":"2021-06-30T06:19:05.802549Z","shell.execute_reply":"2021-06-30T06:19:06.168978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Custom schedule as reccomended by https://www.tensorflow.org/text/tutorials/transformer#create_the_transformer","metadata":{}},{"cell_type":"code","source":"d_model = 32\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n    \nlearning_rate = CustomSchedule(d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)\noptimizer2 = tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.98,epsilon=1e-9)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:27:58.906923Z","iopub.execute_input":"2021-06-30T06:27:58.907405Z","iopub.status.idle":"2021-06-30T06:27:58.927557Z","shell.execute_reply.started":"2021-06-30T06:27:58.90736Z","shell.execute_reply":"2021-06-30T06:27:58.925963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_data = sum(1 for _ in input_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:27:59.852996Z","iopub.execute_input":"2021-06-30T06:27:59.85338Z","iopub.status.idle":"2021-06-30T06:28:00.046513Z","shell.execute_reply.started":"2021-06-30T06:27:59.853351Z","shell.execute_reply":"2021-06-30T06:28:00.045695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We train for 200 iterations\n- In the original implementation the transformer model requires a \"start\" token\n- Here this \"Start\" token is -0.1","metadata":{}},{"cell_type":"code","source":"for e in range(200):\n    for i,batch in enumerate(zip(input_dataset, target_dataset)):\n\n        if i<len_data-1:\n            in1, tar = batch\n            \n            tar_inp = tf.ones((256,1,3))*-0.1\n            tar_inp = tf.concat([tar_inp,tar[:, :-1]],axis=1)\n            tar_real = tar[:, 1:]\n\n            with tf.GradientTape() as tape:\n                predictions = xformer(in1, tar_inp,True)\n                loss = tf.reduce_mean(tf.square(tar_real-predictions[:,:-1]))\n            gradients = tape.gradient(loss, xformer.trainable_variables)\n            optimizer.apply_gradients(zip(gradients, xformer.trainable_variables))\n            \n            with tf.GradientTape() as tape:\n                pred2 = model(in1)\n                loss2 = tf.reduce_mean(tf.square(tar-pred2))\n            gradients = tape.gradient(loss2, model.trainable_variables)\n            optimizer2.apply_gradients(zip(gradients, model.trainable_variables))\n                \n    print(f\"epoch {e}| Transformer:{loss.numpy()} | GRU:{loss2.numpy()}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:28:02.049335Z","iopub.execute_input":"2021-06-30T06:28:02.049698Z","iopub.status.idle":"2021-06-30T06:36:38.432104Z","shell.execute_reply.started":"2021-06-30T06:28:02.049668Z","shell.execute_reply":"2021-06-30T06:36:38.43032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The code below can be used after training to predict the output of a transformer\n- notice that each element of the output is iteratively concatenated\n- the GRU model does not require this step","metadata":{}},{"cell_type":"code","source":"def predict(inp):\n    \n    inp = inp.reshape(-1,24,4)\n#     inp = tf.reshape(inp,(-1,24,4))\n    n_batch = inp.shape[0]\n    start = np.ones((n_batch,1,3))*-0.1\n    \n    for i in range(6):\n        \n        pred = xformer(inp,start,False)\n        pred = np.expand_dims(pred[:,-1,:],0)\n        start = np.concatenate([start,pred],axis=1)\n#         print(start.shape)\n        \n    return start[:,1:]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:36:43.377698Z","iopub.execute_input":"2021-06-30T06:36:43.378037Z","iopub.status.idle":"2021-06-30T06:36:43.384098Z","shell.execute_reply.started":"2021-06-30T06:36:43.378007Z","shell.execute_reply":"2021-06-30T06:36:43.383214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = timeseries_dataset_from_array(X_test,None, sequence_length=sample_length,batch_size=1, sequence_stride=sample_length)\ny_test = timeseries_dataset_from_array(y_test, None, sequence_length=6,batch_size=1, sequence_stride=sample_length)\nx_test = list(x_test.as_numpy_iterator())\ny_test = list(y_test.as_numpy_iterator())","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:36:43.656124Z","iopub.execute_input":"2021-06-30T06:36:43.656446Z","iopub.status.idle":"2021-06-30T06:36:43.82845Z","shell.execute_reply.started":"2021-06-30T06:36:43.656416Z","shell.execute_reply":"2021-06-30T06:36:43.827608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test2 = np.asarray(x_test).reshape(-1,24,4)\ny_test2 = np.asarray(y_test).reshape(-1,6,3)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:36:44.155247Z","iopub.execute_input":"2021-06-30T06:36:44.155568Z","iopub.status.idle":"2021-06-30T06:36:44.16334Z","shell.execute_reply.started":"2021-06-30T06:36:44.155539Z","shell.execute_reply":"2021-06-30T06:36:44.162409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_xform = [predict(x) for x in x_test2]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:36:45.171139Z","iopub.execute_input":"2021-06-30T06:36:45.171499Z","iopub.status.idle":"2021-06-30T06:37:59.423484Z","shell.execute_reply.started":"2021-06-30T06:36:45.17147Z","shell.execute_reply":"2021-06-30T06:37:59.422665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finally, we test both models for their MSE on the Test Set\n- Both models behave similarly in terms of MSE with the transformer being slightly better with 2 transformer layers (test this out by yourself!)\n- In my experiments shown in this notebook, 4 transformer layers (Deeper) will destroy GRUs lol\n\n- I wouldnt use transformers for very long sequences (transformer has a o(n^2) computational and memory complexity)","metadata":{}},{"cell_type":"code","source":"err_xform = np.mean([(y_true-y_pred)**2 for y_true,y_pred in zip(y_test2,preds_xform)])\n\npred_gru = model(x_test2)\nerr_gru = np.mean((pred_gru-y_test2)**2)\n\nerr_xform,err_gru","metadata":{"execution":{"iopub.status.busy":"2021-06-30T06:37:59.424945Z","iopub.execute_input":"2021-06-30T06:37:59.425263Z","iopub.status.idle":"2021-06-30T06:37:59.444433Z","shell.execute_reply.started":"2021-06-30T06:37:59.425227Z","shell.execute_reply":"2021-06-30T06:37:59.443617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}