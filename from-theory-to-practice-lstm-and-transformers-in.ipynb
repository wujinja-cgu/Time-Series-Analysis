{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30474,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <h1 style=\"color:rgb(228, 12, 33); text-align: center;\">From Theory to Practice: LSTM and Transformers in PyTorch</h1>\n\n---\n![image.png](https://discuss.pytorch.org/uploads/default/6415da0424dd66f2f5b134709b92baa59e604c55)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: rgba(100, 108, 116, 0.1); padding: 20px; border-radius: 10px; color: #333; font-family: Arial, sans-serif;\">\n    <p>Welcome to this Kaggle notebook, where we'll dive deep into understanding and implementing Long Short-Term Memory (LSTM) networks using PyTorch, a powerful deep learning framework. But before we delve into the intricacies of LSTM, let's take a moment to understand the basic concepts of time series data, Recurrent Neural Networks (RNNs), and LSTM.</p>\n    <h2 style=\"color:rgb(31, 103, 211);\">Time Series Data</h2>\n    <p>Time series data is a sequence of numerical data points taken at successive equally spaced points in time. These data points are ordered and depend on the previous data points, making time series data a prime candidate for predictions. Examples of time series data include stock prices, weather forecasts, and sales data, among many others.</p>\n    <h2 style=\"color:rgb(31, 103, 211);\">Recurrent Neural Networks (RNNs)</h2>\n    <p>Traditional neural networks struggle with time series data due to their inability to remember previous inputs in their current state. Recurrent Neural Networks (RNNs), however, are designed to address this problem. RNNs are a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows them to use their internal state (memory) to process sequences of inputs, making them ideal for time-dependent data.</p>\n    <p>However, RNNs suffer from certain limitations. They struggle to handle long-term dependencies because of the 'vanishing gradient' problem, where the contribution of information decays geometrically over time, making it difficult for the RNN to learn from earlier layers.</p>\n    <h2 style=\"color:rgb(31, 103, 211);\">Long Short-Term Memory (LSTM)</h2>\n    <p>Long Short-Term Memory networks, or LSTMs, are a special kind of RNN capable of learning long-term dependencies. Introduced by Hochreiter and Schmidhuber in 1997, LSTMs have a unique design that helps combat the vanishing gradient problem. They contain a cell state and three gates (input, forget, and output) to control the flow of information inside the network, allowing them to remember or forget information over long periods of time.</p>\n    <p>In this notebook, we will explore how to correctly implement LSTM in PyTorch and use it for time series prediction tasks. We will cover everything from the basics of LSTM to its implementation, aiming to provide a comprehensive understanding of this powerful neural network architecture. Let's get started!</p>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: rgba(100, 108, 116, 0.1); padding: 20px; border-radius: 10px; color: #333; font-family: Arial, sans-serif;\">\n    <h2 style=\"color:rgb(31, 103, 211);\">Understanding Input and Output in torch.nn.RNN</h2>\n    <p>In this section, we're going to delve into the specifics of the input and output parameters of the torch.nn.RNN module, a built-in recurrent neural network (RNN) implementation in the PyTorch library. It's crucial to understand these parameters to fully leverage PyTorch's RNN capabilities in our LSTM implementation.</p>\n    <h3 style=\"color:rgb(172, 28, 44);\">Input to torch.nn.RNN</h3>\n    <p>The torch.nn.RNN module takes in two primary inputs:</p>\n    <ul>\n        <li><b>input</b>: This represents the sequence that is fed into the network. The expected size is (seq_len, batch, input_size). However, if batch_first=True is specified, then the input size should be rearranged to (batch, seq_len, input_size).</li>\n        <li><b>h_0</b>: This stands for the initial hidden state of the network at time step t=0. By default, if we don't initialize this hidden layer, PyTorch will automatically initialize it with zeros. The size of h_0 should be (num_layers * num_directions, batch, input_size), where num_layers represents the number of stacked RNNs and num_directions equals 2 for bidirectional RNNs and 1 otherwise.</li>\n    </ul>\n    <h3 style=\"color:rgb(172, 28, 44);\">Output from torch.nn.RNN</h3>\n    <p>The torch.nn.RNN module provides two outputs:</p>\n    <ul>\n        <li><b>out</b>: This represents the output from the last RNN layer for all time steps. The size is (seq_len, batch, num_directions * hidden_size). However, if batch_first=True is specified, the output size becomes (batch, seq_len, num_directions * hidden_size).</li>\n        <li><b>h_n</b>: This is the hidden state value from the last time step across all RNN layers. The size is (num_layers * num_directions, batch, hidden_size). Unlike the input, the h_n is unaffected by batch_first=True.</li>\n    </ul>\n    <p>To better visualize these inputs and outputs, refer to the following diagram. In this case, we assume a batch size of 1. While the diagram illustrates an LSTM, which has two hidden parameters (h, c), please note that RNN and GRU only have h.</p>\n    <p>By understanding these parameters, we can harness the power of the torch.nn.RNN module and build effective models for our time series data using LSTM. Let's continue our exploration of LSTM with PyTorch in the following sections.</p>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"![image.png](https://miro.medium.com/max/576/1*tUxl5-C-t3Qumt0cyVhm2g.png)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"ToC\"></a>\n# Table of Contents\n- [1. Imports](#1)\n- [2. LSTM](#2)\n    - [Many-to-One](#2.1)\n    - [Many-to-Many](#2.2)\n    - [Many-to-Many generating sequence](#2.3)\n- [3. Transformers](#3)\n    - [Masking Input](#3.1)\n    - [SOS & EOS tokens](#3.2)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# **<div style=\"padding:10px;color:white;display:fill;border-radius:5px;background-color:rgb(31, 103, 211);font-size:120%;font-family:Verdana;\"><center><span> Imports </span></center></div>**","metadata":{}},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom tqdm import tqdm \nimport numpy as np \nimport pandas as pd \nimport random\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nsns.set_style('white')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #f2f2f2; padding: 20px; border-radius: 10px; color: #333; font-family: Arial, sans-serif;\">\n    <h2 style=\"color:rgb(31, 103, 211);\">About the Dataset</h2>\n    <p>In this notebook, we will utilize a simple time series data to test and understand the application of LSTM and Transformer models. The chosen dataset is quite straightforward â€” a range of numbers starting from 0 and ending at 1000. This simplicity will allow us to focus more on the workings of the LSTM and Transformer models, examining how well they can comprehend and process a simple sequential numerical data. Through this, we aim to achieve a clear understanding of these powerful deep learning techniques.</p>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# **<div style=\"padding:10px;color:white;display:fill;border-radius:5px;background-color:rgb(31, 103, 211);font-size:120%;font-family:Verdana;\"><center><span> LSTM </span></center></div>**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n<div style=\"background-color: #f2f2f2; padding: 20px; border-radius: 10px; color: #333; font-family: Arial, sans-serif;\">\n    <h2 style=\"color:rgb(31, 103, 211);\">Understanding Many-to-One Architecture in LSTM</h2>\n    <p>Long Short-Term Memory (LSTM) networks, like all Recurrent Neural Networks (RNNs), are renowned for their ability to process sequential data. One of the key aspects that make them flexible and powerful is the various types of input-output architectures they can adopt, one of which is the Many-to-One architecture.</p>\n    <p>In a Many-to-One LSTM architecture, the model accepts a sequence of inputs over multiple time steps and produces a single output. In each time step, the LSTM cell takes in an input and the previous cell's hidden state, processes them, and passes on its own hidden state to the next cell.</p>\n    <p>Despite receiving input at each time step, the Many-to-One LSTM only produces its final output at the last time step. This characteristic makes Many-to-One LSTM networks particularly useful for tasks like sentiment analysis, where a model reads a sequence of words (input) and outputs a single sentiment score, or text classification, where a document is read sequentially and a single class label is output.</p>\n    <p>Through the power of LSTM and the flexibility of architectures like Many-to-One, we can effectively tackle a wide range of sequence-based problems in the world of machine learning and artificial intelligence.</p>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"### Create Custom Data Loader [multi-core]","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, seq_len=5, max_len=1000):\n        super(CustomDataset).__init__()\n        self.datalist = np.arange(0,max_len)\n        self.data, self.targets = self.timeseries(self.datalist, seq_len)\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def timeseries(self, data, window):\n        temp = []\n        targ = data[window:]\n        for i in range(len(data)-window):\n            temp.append(data[i:i+window])\n\n        return np.array(temp), targ\n    \n    def __getitem__(self, index):\n        x = torch.tensor(self.data[index]).type(torch.Tensor)\n        y = torch.tensor(self.targets[index]).type(torch.Tensor)\n        return x,y\n    \ndataset = CustomDataset(seq_len=5, max_len=1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x,y in dataset:\n    print(x,y)\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)\n#collate_fn=custom_collector","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x,y in dataloader:\n    print(x,y)\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #f2f2f2; padding: 20px; border-radius: 10px; color: #333; font-family: Arial, sans-serif;\">\n<p>Let's take a closer look at our specific use case for the many-to-one LSTM architecture. In our scenario, we are feeding the LSTM with a sequence of 5 random numbers, and we anticipate that the model will predict the 6th number in the sequence. While we've chosen a straightforward series of incrementing numbers for this example, the potential applications of this concept extend much further.</p>\n\n<p style=\"color:rgb(172, 28, 44);\">Imagine this sequence being a time-series data of stock prices, weather conditions, or even a series of steps in a logical reasoning question. The ability to predict the next event based on a series of preceding events is a critical aspect in many fields, including finance, meteorology, and artificial intelligence. By training our LSTM model to understand and predict these sequences, we can leverage the many-to-one LSTM architecture to solve complex problems in these areas and beyond.</p>\n\n</div>","metadata":{}},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.lstm = nn.LSTM(input_size,hidden_size,num_layers,batch_first=True)\n        self.fc = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        # hidden states not defnined hence the value of h0,c0 == (0,0)\n        out, (hn, cn) = self.lstm(x)\n        \n        # as the diagram suggest to take the last output in many to one \n        # print(out.shape) \n        # print(hn.shape)\n        # all batch, last column of seq, all hidden values\n        out = out[:, -1, :]\n        out = self.fc(out)\n        \n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RNN(input_size=1, hidden_size=256, num_layers=2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = torch.tensor([11,12,13,14,15]).type(torch.Tensor).view(1,-1,1)\nt.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model(t)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training ","metadata":{}},{"cell_type":"code","source":"loss_function = nn.MSELoss()\nlearning_rate = 1e-3\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for e in tqdm(range(50)):\n    i = 0\n    for x,y in dataloader:\n        optimizer.zero_grad()\n\n        x = torch.unsqueeze(x, 0).permute(1,2,0)\n        # forward\n        predictions = model(x)\n\n        loss = loss_function(predictions.view(-1), y)\n        \n        # backward\n        loss.backward()\n\n        # optimization\n        optimizer.step()\n\n        i+=1\n    if e%5==0:\n        print(loss.detach().numpy())","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tensor = torch.tensor([10,11,12,13,14]).type(torch.Tensor).view(1,-1,1)\nmodel(input_tensor)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n<div style=\"background-color: #f2f2f2; padding: 20px; border-radius: 10px; color: #333; font-family: Arial, sans-serif;\">\n    <h2 style=\"color:rgb(31, 103, 211);\">Understanding Many-to-Many Architecture in LSTM</h2>\n   <p>Another crucial architecture in the world of Long Short-Term Memory (LSTM) networks, a type of Recurrent Neural Network (RNN), is the Many-to-Many architecture. This architecture offers a versatile way of handling a diverse set of problems involving sequential data.</p>\n    <p>In a Many-to-Many LSTM architecture, the model processes a sequence of inputs over multiple time steps and generates a sequence of outputs. In this setting, each LSTM cell takes in an input and the previous cell's hidden state at each time step, then produces an output along with its own hidden state that it passes on to the next cell.</p>\n    <p>Unlike the Many-to-One LSTM, the Many-to-Many LSTM doesn't wait till the last time step to produce an output. Instead, it generates an output at each time step. This makes Many-to-Many LSTM networks highly useful for tasks such as machine translation, where a sequence of words in one language (input) is translated into a sequence of words in another language (output).</p>\n    <p>The Many-to-Many architecture of LSTM opens up a broad array of possibilities, making it a powerful tool in the realms of machine learning and artificial intelligence.</p>\n</div>","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, seq_len=50, future=5,  max_len=1000):\n        super(CustomDataset).__init__()\n        self.datalist = np.arange(0,max_len)\n        self.data, self.targets = self.timeseries(self.datalist, seq_len, future)\n        \n    def __len__(self):\n        #this len will decide the index range in getitem\n        return len(self.targets)\n    \n    def timeseries(self, data, window, future):\n        temp = []\n        targ = []\n        \n        for i in range(len(data)-window):\n            temp.append(data[i:i+window])\n            \n        for i in range(len(data)-window -future):\n            targ.append(data[i+window:i+window+future])\n\n        return np.array(temp), targ\n    \n    def __getitem__(self, index):\n        x = torch.tensor(self.data[index]).type(torch.Tensor)\n        y = torch.tensor(self.targets[index]).type(torch.Tensor)\n        return x,y\n    \ndataset = CustomDataset(seq_len=50, future=5, max_len=1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x,y in dataset:\n    print(x.shape, y.shape)\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n#collate_fn=custom_collector","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x,y in dataloader:\n    print(x.shape, y.shape)\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, future=5):\n        super().__init__()\n        self.future = future\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.lstm = nn.LSTM(input_size,hidden_size,num_layers,batch_first=True)\n        self.fc = nn.Linear(hidden_size, future)\n\n    def forward(self, x):\n        # hidden states not defnined hence the value of h0,c0 == (0,0)\n        out, (hn, cn) = self.lstm(x)\n        \nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, future=5):\n        super().__init__()\n        self.future = future\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.lstm = nn.LSTM(input_size,hidden_size,num_layers,batch_first=True)\n        self.fc = nn.Linear(hidden_size, future)\n\n    def forward(self, x):\n        # hidden states not defnined hence the value of h0,c0 == (0,0)\n        out, (hn, cn) = self.lstm(x)\n        \n        # as the diagram suggest to take the last output in many to one \n        # print(out.shape) \n        # print(hn.shape)\n        # all batch, last column of seq, all hidden values\n        out = out[:, -self.future, :]\n        out = self.fc(out)\n        \n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RNN(input_size=1, hidden_size=256, num_layers=2, future=5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = 45\nt = torch.tensor(np.arange(d,d+50)).type(torch.Tensor).view(1,-1,1)\nt.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model(t)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_function = nn.MSELoss()\nlearning_rate = 1e-3\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for e in tqdm(range(50)):\n    i = 0\n    avg_loss = []\n    for x,y in dataloader:\n        optimizer.zero_grad()\n\n        x = torch.unsqueeze(x, 0).permute(1,2,0)\n        # forward\n        predictions = model(x)\n        \n        # loss\n        loss = loss_function(predictions, y)\n        \n        # backward\n        loss.backward()\n\n        # optimization\n        optimizer.step()\n        avg_loss.append(loss.detach().numpy())\n\n        i+=1\n    if e%2==0:\n        avg_loss = np.array(avg_loss)\n        print(avg_loss.mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #f2f2f2; padding: 20px; border-radius: 10px; color: #333; font-family: Arial, sans-serif;\">\n<p>After feeding the initial 50 terms of our sequence into the model, we begin to observe some promising results. It appears that the model is successfully learning to recognize the underlying patterns in the sequence.</p>\n<p>The output generated by the model seems to adhere to the logic of the sequence, suggesting that the LSTM architecture is effectively capturing and understanding the sequential dependencies. This ability to discern patterns and extrapolate them is a powerful aspect of LSTM networks, and it's rewarding to see it at work in our model.</p>\n<p>These early results are encouraging, indicating that our model is on the right track. As we continue to refine and train our LSTM, we can expect it to become even more adept at understanding and predicting the sequence.</p>\n</div>","metadata":{}},{"cell_type":"code","source":"d = random.randint(0,1000)\nt = torch.tensor(np.arange(d,d+50)).type(torch.Tensor).view(1,-1,1)\nr = model(t).view(-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(16,4))\nplt_x = np.arange(0,t.shape[1]+len(r))\nplt_y = np.arange(d,d+50+len(r))\n\nplt_xp = np.arange(t.shape[1], t.shape[1]+len(r))\nplt_yp = r.detach().numpy()\nfor i in range(len(r)):\n    plt.scatter(plt_x, plt_y)\n    plt.scatter(plt_xp, plt_yp)\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n<div style=\"background-color: #f2f2f2; padding: 20px; border-radius: 10px; color: #333; font-family: Arial, sans-serif;\">\n    <h2 style=\"color:rgb(31, 103, 211);\">Understanding Many-to-Many Sequence Generation with LSTM</h2>\n    <p>When working with Long Short-Term Memory (LSTM) networks, it's essential to understand how sequence generation is handled, particularly in a Many-to-Many setting. In such an architecture, the output from each LSTM cell can be used as an input to a subsequent feed-forward network to generate a sequence of outputs.</p>\n    <p>Let's consider the following block of code as an example:</p>\n    <pre style=\"background-color: #e0e0e0; padding: 10px; border-radius: 10px;\">\n        out, (hn, cn) = self.lstm(x)\n        res = torch.zeros((out.shape[0], out.shape[1]))\n        for b in range(out.shape[0]):\n            feed = out[b, :, :]\n            _out = self.fc(feed).view(-1)\n            res[b] = _out\n    </pre>\n    <p>In this code, <code>self.lstm(x)</code> applies the LSTM layer to the input <code>x</code>, generating an output <code>out</code> and the final hidden and cell states <code>hn</code> and <code>cn</code>. We then initialize a zeros tensor <code>res</code> of the same size as <code>out</code> to store our results.</p>\n    <p>Then, for each sequence in the output <code>out</code>, we feed the sequence through a fully connected layer <code>self.fc(feed)</code> and reshape the output to match our expected dimensions using <code>.view(-1)</code>. The result is stored in the corresponding position in <code>res</code>.</p>\n    <p>This process exemplifies how a Many-to-Many LSTM network can be used to generate a sequence of outputs, with the LSTM layer and a subsequent feed-forward layer working in tandem to transform a sequence of inputs into a corresponding sequence of outputs.</p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, seq_len=50, future=50,  max_len=1000):\n        super(CustomDataset).__init__()\n        self.datalist = np.arange(0,max_len)\n        self.data, self.targets = self.timeseries(self.datalist, seq_len, future)\n        \n    def __len__(self):\n        #this len will decide the index range in getitem\n        return len(self.targets)\n    \n    def timeseries(self, data, window, future):\n        temp = []\n        targ = []\n        \n        for i in range(len(data)-window):\n            temp.append(data[i:i+window])\n            \n        for i in range(len(data)-window -future):\n            targ.append(data[i+future:i+window+future])\n\n        return np.array(temp), targ\n    \n    def __getitem__(self, index):\n        x = torch.tensor(self.data[index]).type(torch.Tensor)\n        y = torch.tensor(self.targets[index]).type(torch.Tensor)\n        return x,y\n    \ndataset = CustomDataset(seq_len=50, future=5, max_len=1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x, y in dataset:\n    print(x.shape, y.shape)\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n#collate_fn=custom_collector","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, future=5):\n        super().__init__()\n        self.future = future\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        self.lstm = nn.LSTM(input_size,hidden_size,num_layers,batch_first=True)\n        self.fc = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        # hidden states not defnined hence the value of h0,c0 == (0,0)\n        out, (hn, cn) = self.lstm(x)\n        \n        # as the diagram suggest to take the last output in many to one \n        # print(out.shape)\n        # print(hn.shape)\n        # all batch, last column of seq, all hidden values\n        res = torch.zeros((out.shape[0], out.shape[1]))\n        for b in range(out.shape[0]):\n            feed = out[b, :, :]\n            _out = self.fc(feed).view(-1)\n            res[b] = _out\n        \n        return res","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RNN(input_size=1, hidden_size=256, num_layers=2, future=5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = torch.tensor(np.arange(d,d+50)).type(torch.Tensor).view(1,-1,1)\nr = model(t).view(-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_function = nn.MSELoss()\nlearning_rate = 1e-3\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for e in tqdm(range(100)):\n    i = 0\n    avg_loss = []\n    for x,y in dataloader:\n        optimizer.zero_grad()\n\n        x = torch.unsqueeze(x, 0).permute(1,2,0)\n        # forward\n        predictions = model(x)\n        \n        # loss\n        loss = loss_function(predictions, y)\n        \n        # backward\n        loss.backward()\n\n        # optimization\n        optimizer.step()\n        avg_loss.append(loss.detach().numpy())\n\n        i+=1\n        \n    if e%5==0:\n        avg_loss = np.array(avg_loss)\n        print(avg_loss.mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = random.randint(0,1000)\nt = torch.tensor(np.arange(d,d+50)).type(torch.Tensor).view(1,-1,1)\nr = model(t).view(-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(16,4))\nplt_x = np.arange(0,t.shape[1])\nplt_y = np.arange(d,d+50)\n\nplt_xp = np.arange(5, t.shape[1]+5)\nplt_yp = r.detach().numpy()\nfor i in range(len(r)):\n    plt.scatter(plt_x, plt_y, label=\"real\")\n    plt.scatter(plt_xp, plt_yp, label=\"predicted\")\n    \nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# **<div style=\"padding:10px;color:white;display:fill;border-radius:5px;background-color:rgb(31, 103, 211);font-size:120%;font-family:Verdana;\"><center><span> Transformers </span></center></div>**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.3\"></a>\n<div style=\"background-color: #f2f2f2; padding: 20px; border-radius: 10px; color: #333; font-family: Arial, sans-serif;\">\n    <p style=\"color:rgb(172, 28, 44);\">Transformers, a breakthrough in the field of natural language processing, also adopt various types of input-output architectures, including the Many-to-Many setup. In this context, Transformers bring a unique approach to the table, contrasting with the methods used in traditional Recurrent Neural Networks (RNNs) such as LSTM.</p>\n    <p>In a Many-to-Many Transformer architecture, the model accepts a sequence of inputs and returns a sequence of outputs. However, unlike RNNs, which process sequences in a time-stepped manner, Transformers process all inputs simultaneously. This is made possible by the attention mechanism, which allows the model to focus on different parts of the input sequence for each output, essentially creating a 'shortcut' between each input and output.</p>\n    <p>This architecture is especially useful in tasks like machine translation, where the model needs to understand the context of the whole sentence to accurately translate it. Similarly, it can be used in tasks like text summarization or question answering, where understanding the entire context at once can lead to better results.</p>\n    <p>The Many-to-Many architecture in Transformers, combined with their attention mechanism, offers an innovative approach to tackling sequential tasks, making Transformers a powerful tool in the field of machine learning and artificial intelligence.</p>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"\n![image.png](https://images.deepai.org/converted-papers/2001.08317/x1.png)","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, seq_len=50, future=50,  max_len=1000):\n        super(CustomDataset).__init__()\n        \n        self.vocab = {'SOS':1001, 'EOS':1002}\n        self.datalist = np.arange(0,max_len)\n        self.data, self.targets = self.timeseries(self.datalist, seq_len, future)\n        \n    def __len__(self):\n        #this len will decide the index range in getitem\n        return len(self.targets)\n    \n    def timeseries(self, data, window, future):\n        temp = []\n        targ = []\n        \n        for i in range(len(data)-window):\n            temp.append(data[i:i+window])\n            \n        for i in range(len(data)-window -future):\n            targ.append(data[i+future:i+window+future])\n\n        return np.array(temp), targ\n    \n    def __getitem__(self, index):\n        x = torch.tensor(self.data[index]).type(torch.Tensor)\n        x = torch.cat((torch.tensor([self.vocab['SOS']]), x, torch.tensor([self.vocab['EOS']]))).type(torch.LongTensor)\n        \n        y = torch.tensor(self.targets[index]).type(torch.Tensor)\n        y = torch.cat((torch.tensor([self.vocab['SOS']]), y, torch.tensor([self.vocab['EOS']]))).type(torch.LongTensor)\n        \n        return x,y\n    \ndataset = CustomDataset(seq_len=48, future=5, max_len=1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x, y in dataset:\n    print(x)\n    print(y)\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n#collate_fn=custom_collector","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x, y in dataloader:\n    print(x.shape)\n    print(y.shape)\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n<div style=\"background-color: #f2f2f2; padding: 20px; border-radius: 10px; color: #333; font-family: Arial, sans-serif;\">\n    <h3 style=\"color:rgb(172, 28, 44);\">The Power of Masking and Efficiency in Transformers</h3>\n    <p>One of the remarkable features of Transformers is their use of masking during the training process. Masking is an essential aspect of the Transformer's architecture that prevents the model from seeing future tokens in the input sequence during training, thereby preserving the sequential nature of the language.</p>\n    <p>In a task such as language translation, where the input sequence is fed into the model all at once, it's crucial that the prediction for each word doesn't rely on words that come after it in the sequence. This is achieved by applying a mask to the input that effectively hides future words from the model during the training phase.</p>\n    <p>Not only does masking maintain the sequential integrity of the language, but it also allows Transformers to train more efficiently than their RNN counterparts, like LSTM. Unlike RNNs, which process sequences step-by-step and thus require longer training times for long sequences, Transformers can process all the tokens in the sequence simultaneously, thanks to their attention mechanism. This parallel processing significantly speeds up the training process and allows the model to handle longer sequences more effectively.</p>\n    <p>Thus, through the use of masking and their unique architecture, Transformers manage to overcome some of the limitations of traditional RNNs, offering a more efficient and effective approach to sequence-based tasks in machine learning and artificial intelligence.</p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, num_tokens, dim_model, num_heads, num_layers, input_seq):\n        super().__init__()\n        self.input_seq = input_seq\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(num_tokens, dim_model)\n        self.transformer = nn.Transformer(d_model=dim_model, nhead=num_heads,  \n                                          num_encoder_layers=3, num_decoder_layers=3, \n                                          dim_feedforward=256, batch_first=True)\n        \n        self.fc = nn.Linear(dim_model, num_tokens)\n\n    def forward(self, src, tgt, tf=True):\n        mask = self.get_mask(tgt.shape[1], teacher_force=tf)\n        src = self.embedding(src) \n        tgt = self.embedding(tgt)\n        \n        out = self.transformer(src, tgt, tgt_mask=mask)\n        feed = self.fc(out)\n        feed = torch.squeeze(feed,2)\n        \n        return feed\n            \n            \n    def get_mask(self, size, teacher_force=True):\n        if teacher_force:\n            mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n            mask = mask.float()\n            mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n            mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n\n            # EX for size=5:\n            # [[0., -inf, -inf, -inf, -inf],\n            #  [0.,   0., -inf, -inf, -inf],\n            #  [0.,   0.,   0., -inf, -inf],\n            #  [0.,   0.,   0.,   0., -inf],\n            #  [0.,   0.,   0.,   0.,   0.]]\n\n            return mask\n        else:\n            mask = torch.tril(torch.zeros(size, size) == 1) # Lower triangular matrix\n            mask = mask.float()\n            mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n            mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n\n            return mask","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Transformer(num_tokens=1000+3, dim_model=32, num_heads=2, num_layers=2, input_seq=50)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.shape, y.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model(x, y).shape ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = model(x,y)\nt.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t.permute(0,2,1).shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_function = nn.CrossEntropyLoss()\nlearning_rate = 1e-3\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for e in tqdm(range(25)):\n    i = 0\n    avg_loss = []\n    for x,y in dataloader:\n        optimizer.zero_grad()\n        \n        #one step behind input and output // Like language modeling \n        y_input = y[:, :-1]         # from starting to -1 position\n        y_expected = y[:, 1:]       # from 1st position to last \n        # this is done so that in prediction we see a start of token \n        \n        # forward\n        predictions = model(x, y_input)\n        pred = predictions.permute(0, 2, 1)\n        \n        # loss\n        loss = loss_function(pred, y_expected)\n        \n        # backward\n        loss.backward()\n\n        # optimization\n        optimizer.step()\n        avg_loss.append(loss.detach().numpy())\n\n        i+=1\n        \n    if e%5==0:\n        avg_loss = np.array(avg_loss)\n        print(avg_loss.mean())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.squeeze(predictions.topk(1).indices, 2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_expected ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.argmax(pred, dim=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n<div style=\"background-color: #f2f2f2; padding: 20px; border-radius: 10px; color: #333; font-family: Arial, sans-serif;\">\n    <h3 style=\"color:rgb(172, 28, 44);\">The Role of SOS and EOS Tokens in Transformers</h3>\n    <p>In the domain of natural language processing, particularly when working with Transformer models, special tokens like Start of Sentence (SOS) and End of Sentence (EOS) play a crucial role. These tokens provide valuable cues about the boundaries of sentences, facilitating the model's understanding of language structure.</p>\n    <p>The SOS token is added at the beginning of each sentence, marking its start. Similarly, the EOS token is appended at the end of each sentence to indicate its conclusion. These tokens serve as consistent markers that help the model identify and process sentences as distinct units within larger bodies of text.</p>\n    <p>Furthermore, in the context of sequence generation tasks, these tokens play an essential role in determining when to begin and end the generation process. For example, during text generation, an EOS token indicates to the model that it should stop generating further tokens.</p>\n    <p>Therefore, SOS and EOS tokens are more than just markers; they're integral components in the design and functioning of Transformer models, contributing significantly to their ability to effectively understand and generate human language.</p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"def predict(model, input_sequence, max_length=50, SOS_token=1000+1, EOS_token=1000+2):\n    model.eval()\n    \n    input_sequence = torch.tensor(input_sequence)\n    input_sequence = torch.cat((torch.tensor([SOS_token]), input_sequence, torch.tensor([EOS_token]))).type(torch.LongTensor) \n    input_sequence = torch.unsqueeze(input_sequence,0)\n    \n    y_input = torch.tensor([1001], dtype=torch.long)\n    y_input = torch.unsqueeze(y_input,0)\n\n    for _ in range(max_length):\n        \n        predictions = model(input_sequence, y_input)\n        \n        top = predictions.topk(1).indices\n        top = torch.squeeze(top, 2)\n        \n        next_item = torch.unsqueeze(top[:,-1],0)\n        y_input = torch.cat((y_input, next_item), dim=1)\n        mask = model.get_mask(y_input.shape[1])\n        if next_item == EOS_token:\n            break\n\n    return y_input.view(-1).tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = random.randint(0,900)\nt = torch.tensor(np.arange(d,d+48)).type(torch.Tensor)\ninput_sequence = t\nprint(t)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r=predict(model, input_sequence)\nprint(r)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(16,4))\n\nplt_x = np.arange(0,t.shape[0])\nplt_y = t\n\nplt_xp = np.arange(5, t.shape[0]+5)\nplt_yp = r[1:-2]\n\nplt.scatter(plt_x, plt_y, s=14, color='r', label=\"real\")\nplt.scatter(plt_xp, plt_yp, s=7, color='b', label=\"predicted\")\n    \nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}